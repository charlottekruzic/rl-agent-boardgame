{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook permettant d'entrainer nos agents RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from gym_env_2dim import LabyrinthEnv, RewardLoggingCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = LabyrinthEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace des actions :  MultiDiscrete([ 4 12])\n",
      "Espace des observations :  Box(0.0, 1.0, (245,), float32)\n"
     ]
    }
   ],
   "source": [
    "# Affichage des actions possibles et de l'espace d'observation\n",
    "print(\"Espace des actions : \", env.action_space)\n",
    "print(\"Espace des observations : \", env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./logs/\"\n",
    "env = Monitor(LabyrinthEnv(), log_dir)\n",
    "\n",
    "# Callback d'évaluation pour enregistrer la récompense moyenne\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=5000,\n",
    "                             deterministic=True, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./logs/PPO_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\charl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.joueur_actuel to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.joueur_actuel` for environment variables or `env.get_wrapper_attr('joueur_actuel')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "| reward/            |       |\n",
      "|    player_1        | [-1.] |\n",
      "| time/              |       |\n",
      "|    fps             | 1397  |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 1     |\n",
      "|    total_timesteps | 2048  |\n",
      "------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.42e+03    |\n",
      "|    ep_rew_mean          | -4e+03      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 806         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012208093 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.86       |\n",
      "|    explained_variance   | 0.000213    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 164         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    value_loss           | 691         |\n",
      "-----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=5000, episode_reward=-10803.80 +/- 1543.10\n",
      "Episode length: 13086.20 +/- 3103.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.31e+04     |\n",
      "|    mean_reward          | -1.08e+04    |\n",
      "| reward/                 |              |\n",
      "|    player_1             | [0.]         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0092821345 |\n",
      "|    clip_fraction        | 0.0892       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.84        |\n",
      "|    explained_variance   | -5.75e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 301          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0224      |\n",
      "|    value_loss           | 915          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| reward/            |           |\n",
      "|    player_1        | [-1.]     |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 2.2e+03   |\n",
      "|    ep_rew_mean     | -3.64e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 113       |\n",
      "|    iterations      | 3         |\n",
      "|    time_elapsed    | 54        |\n",
      "|    total_timesteps | 6144      |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| reward/                 |            |\n",
      "|    player_1             | [-1.]      |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2.2e+03    |\n",
      "|    ep_rew_mean          | -3.64e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 134        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 60         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00850766 |\n",
      "|    clip_fraction        | 0.0887     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.81      |\n",
      "|    explained_variance   | 3.46e-05   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 219        |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.025     |\n",
      "|    value_loss           | 521        |\n",
      "----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=10000, episode_reward=-6293.40 +/- 2730.95\n",
      "Episode length: 12332.80 +/- 4991.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.23e+04    |\n",
      "|    mean_reward          | -6.29e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010118462 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.76       |\n",
      "|    explained_variance   | 7.75e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 224         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    value_loss           | 733         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| reward/            |           |\n",
      "|    player_1        | [0.]      |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 2.98e+03  |\n",
      "|    ep_rew_mean     | -4.86e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 99        |\n",
      "|    iterations      | 5         |\n",
      "|    time_elapsed    | 103       |\n",
      "|    total_timesteps | 10240     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-10.]      |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.98e+03    |\n",
      "|    ep_rew_mean          | -4.86e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008167818 |\n",
      "|    clip_fraction        | 0.0813      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.76       |\n",
      "|    explained_variance   | -4.65e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 269         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    value_loss           | 567         |\n",
      "-----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.01e+03    |\n",
      "|    ep_rew_mean          | -4.6e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010594234 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.74       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 162         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0362     |\n",
      "|    value_loss           | 368         |\n",
      "-----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=15000, episode_reward=-4599.80 +/- 2007.53\n",
      "Episode length: 9276.00 +/- 3959.88\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.28e+03   |\n",
      "|    mean_reward          | -4.6e+03   |\n",
      "| reward/                 |            |\n",
      "|    player_1             | [-1.]      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 15000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01420601 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.71      |\n",
      "|    explained_variance   | -2.62e-06  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 52.3       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.041     |\n",
      "|    value_loss           | 147        |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| reward/            |          |\n",
      "|    player_1        | [-10.]   |\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.01e+03 |\n",
      "|    ep_rew_mean     | -4.6e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 122      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 133      |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| reward/                 |            |\n",
      "|    player_1             | [0.]       |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.01e+03   |\n",
      "|    ep_rew_mean          | -4.6e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 135        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 136        |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01074003 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.62      |\n",
      "|    explained_variance   | -7.15e-07  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 74.3       |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0372    |\n",
      "|    value_loss           | 233        |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=20000, episode_reward=-4783.20 +/- 2995.30\n",
      "Episode length: 8023.40 +/- 4705.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.02e+03    |\n",
      "|    mean_reward          | -4.78e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015359707 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.62       |\n",
      "|    explained_variance   | 4.17e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 152         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0397     |\n",
      "|    value_loss           | 233         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| reward/            |          |\n",
      "|    player_1        | [-1.]    |\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.01e+03 |\n",
      "|    ep_rew_mean     | -4.6e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 123      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 165      |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.01e+03    |\n",
      "|    ep_rew_mean          | -4.6e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 174         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029624011 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.6        |\n",
      "|    explained_variance   | 1.31e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.4        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0382     |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.01e+03    |\n",
      "|    ep_rew_mean          | -4.6e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 183         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019919593 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.52       |\n",
      "|    explained_variance   | 1.37e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 64.8        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    value_loss           | 86.4        |\n",
      "-----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=25000, episode_reward=-6017.20 +/- 2222.97\n",
      "Episode length: 10984.40 +/- 3215.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.1e+04     |\n",
      "|    mean_reward          | -6.02e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013729887 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.47       |\n",
      "|    explained_variance   | 7.15e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 63.5        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    value_loss           | 110         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| reward/            |          |\n",
      "|    player_1        | [-1.]    |\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.01e+03 |\n",
      "|    ep_rew_mean     | -4.6e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 111      |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 239      |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.01e+03    |\n",
      "|    ep_rew_mean          | -4.6e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 248         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019017722 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.43       |\n",
      "|    explained_variance   | 5.36e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 61.2        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0415     |\n",
      "|    value_loss           | 68.8        |\n",
      "-----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=30000, episode_reward=-5631.40 +/- 1129.74\n",
      "Episode length: 9189.60 +/- 1613.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.19e+03     |\n",
      "|    mean_reward          | -5.63e+03    |\n",
      "| reward/                 |              |\n",
      "|    player_1             | [-1.]        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0154272625 |\n",
      "|    clip_fraction        | 0.196        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.4         |\n",
      "|    explained_variance   | 1.85e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42           |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0391      |\n",
      "|    value_loss           | 51.4         |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| reward/            |           |\n",
      "|    player_1        | [10.]     |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 3.17e+03  |\n",
      "|    ep_rew_mean     | -4.28e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 106       |\n",
      "|    iterations      | 15        |\n",
      "|    time_elapsed    | 287       |\n",
      "|    total_timesteps | 30720     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.17e+03    |\n",
      "|    ep_rew_mean          | -4.28e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 290         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013256839 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.37       |\n",
      "|    explained_variance   | 8.34e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 69.3        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0361     |\n",
      "|    value_loss           | 100         |\n",
      "-----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.38e+03    |\n",
      "|    ep_rew_mean          | -4.24e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 118         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 293         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009475754 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.27       |\n",
      "|    explained_variance   | 1.4e-05     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.6        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0365     |\n",
      "|    value_loss           | 47.5        |\n",
      "-----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=35000, episode_reward=-7090.20 +/- 780.78\n",
      "Episode length: 13725.20 +/- 1489.73\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.37e+04     |\n",
      "|    mean_reward          | -7.09e+03    |\n",
      "| reward/                 |              |\n",
      "|    player_1             | [-10.]       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 35000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064473506 |\n",
      "|    clip_fraction        | 0.0608       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.26        |\n",
      "|    explained_variance   | 0.0302       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.8         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0157      |\n",
      "|    value_loss           | 75.4         |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| reward/            |           |\n",
      "|    player_1        | [-1.]     |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 3.38e+03  |\n",
      "|    ep_rew_mean     | -4.24e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 109       |\n",
      "|    iterations      | 18        |\n",
      "|    time_elapsed    | 337       |\n",
      "|    total_timesteps | 36864     |\n",
      "----------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.33e+03    |\n",
      "|    ep_rew_mean          | -3.9e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 346         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008599284 |\n",
      "|    clip_fraction        | 0.0828      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.19       |\n",
      "|    explained_variance   | -0.111      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.01        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    value_loss           | 11.4        |\n",
      "-----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=40000, episode_reward=-7459.60 +/- 2548.22\n",
      "Episode length: 14847.80 +/- 5063.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.48e+04     |\n",
      "|    mean_reward          | -7.46e+03    |\n",
      "| reward/                 |              |\n",
      "|    player_1             | [0.]         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072910134 |\n",
      "|    clip_fraction        | 0.0555       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.19        |\n",
      "|    explained_variance   | -0.0503      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.85         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0196      |\n",
      "|    value_loss           | 27.8         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| reward/            |          |\n",
      "|    player_1        | [0.]     |\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.33e+03 |\n",
      "|    ep_rew_mean     | -3.9e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 91       |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 446      |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.33e+03    |\n",
      "|    ep_rew_mean          | -3.9e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 455         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008512059 |\n",
      "|    clip_fraction        | 0.0657      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.17       |\n",
      "|    explained_variance   | -0.126      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.13        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 24.8        |\n",
      "-----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=45000, episode_reward=-6064.40 +/- 1484.44\n",
      "Episode length: 12198.20 +/- 3064.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.22e+04     |\n",
      "|    mean_reward          | -6.06e+03    |\n",
      "| reward/                 |              |\n",
      "|    player_1             | [-10.]       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 45000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060103238 |\n",
      "|    clip_fraction        | 0.0452       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.09        |\n",
      "|    explained_variance   | 0.115        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.6         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.0175      |\n",
      "|    value_loss           | 19.3         |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| reward/            |           |\n",
      "|    player_1        | [0.]      |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 3.53e+03  |\n",
      "|    ep_rew_mean     | -3.86e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 86        |\n",
      "|    iterations      | 22        |\n",
      "|    time_elapsed    | 519       |\n",
      "|    total_timesteps | 45056     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| reward/                 |              |\n",
      "|    player_1             | [0.]         |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.53e+03     |\n",
      "|    ep_rew_mean          | -3.86e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 89           |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 527          |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051572192 |\n",
      "|    clip_fraction        | 0.0492       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.07        |\n",
      "|    explained_variance   | -0.0689      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.7          |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.0153      |\n",
      "|    value_loss           | 41.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.53e+03    |\n",
      "|    ep_rew_mean          | -3.86e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 536         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010644609 |\n",
      "|    clip_fraction        | 0.0979      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3          |\n",
      "|    explained_variance   | -0.578      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.58        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    value_loss           | 14.2        |\n",
      "-----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=50000, episode_reward=-6800.60 +/- 630.40\n",
      "Episode length: 13802.80 +/- 1317.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.38e+04    |\n",
      "|    mean_reward          | -6.8e+03    |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010143921 |\n",
      "|    clip_fraction        | 0.0969      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.99       |\n",
      "|    explained_variance   | -0.0113     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.91        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    value_loss           | 11          |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| reward/            |           |\n",
      "|    player_1        | [-1.]     |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 3.53e+03  |\n",
      "|    ep_rew_mean     | -3.86e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 87        |\n",
      "|    iterations      | 25        |\n",
      "|    time_elapsed    | 584       |\n",
      "|    total_timesteps | 51200     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| reward/                 |              |\n",
      "|    player_1             | [-1.]        |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.53e+03     |\n",
      "|    ep_rew_mean          | -3.86e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 90           |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 587          |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0094705075 |\n",
      "|    clip_fraction        | 0.0833       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.91        |\n",
      "|    explained_variance   | -0.169       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.71         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.0328      |\n",
      "|    value_loss           | 12.4         |\n",
      "------------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=55000, episode_reward=-7147.00 +/- 3080.67\n",
      "Episode length: 14502.60 +/- 6175.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.45e+04    |\n",
      "|    mean_reward          | -7.15e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 55000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008135867 |\n",
      "|    clip_fraction        | 0.0671      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.92       |\n",
      "|    explained_variance   | 0.029       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 61.2        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0217     |\n",
      "|    value_loss           | 44.2        |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| reward/            |           |\n",
      "|    player_1        | [0.]      |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 3.53e+03  |\n",
      "|    ep_rew_mean     | -3.86e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 88        |\n",
      "|    iterations      | 27        |\n",
      "|    time_elapsed    | 621       |\n",
      "|    total_timesteps | 55296     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.53e+03    |\n",
      "|    ep_rew_mean          | -3.86e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 625         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010140616 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.95       |\n",
      "|    explained_variance   | 0.151       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.36        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.033      |\n",
      "|    value_loss           | 13.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.53e+03    |\n",
      "|    ep_rew_mean          | -3.86e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 627         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011866909 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | -0.221      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.28        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0332     |\n",
      "|    value_loss           | 13.3        |\n",
      "-----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=60000, episode_reward=-7293.20 +/- 2061.96\n",
      "Episode length: 14693.60 +/- 4105.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.47e+04     |\n",
      "|    mean_reward          | -7.29e+03    |\n",
      "| reward/                 |              |\n",
      "|    player_1             | [-1.]        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0133176595 |\n",
      "|    clip_fraction        | 0.149        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.91        |\n",
      "|    explained_variance   | -0.199       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.78         |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.0421      |\n",
      "|    value_loss           | 16.7         |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| reward/            |           |\n",
      "|    player_1        | [0.]      |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 3.53e+03  |\n",
      "|    ep_rew_mean     | -3.86e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 88        |\n",
      "|    iterations      | 30        |\n",
      "|    time_elapsed    | 695       |\n",
      "|    total_timesteps | 61440     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.53e+03    |\n",
      "|    ep_rew_mean          | -3.86e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 698         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018320613 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.77       |\n",
      "|    explained_variance   | -0.306      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.12        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0496     |\n",
      "|    value_loss           | 4.6         |\n",
      "-----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=65000, episode_reward=-6491.40 +/- 1272.14\n",
      "Episode length: 13224.80 +/- 2555.93\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.32e+04    |\n",
      "|    mean_reward          | -6.49e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 65000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014830922 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | -0.0397     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.16        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.042      |\n",
      "|    value_loss           | 5.97        |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| reward/            |           |\n",
      "|    player_1        | [0.]      |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 3.53e+03  |\n",
      "|    ep_rew_mean     | -3.86e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 89        |\n",
      "|    iterations      | 32        |\n",
      "|    time_elapsed    | 730       |\n",
      "|    total_timesteps | 65536     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.53e+03    |\n",
      "|    ep_rew_mean          | -3.86e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 733         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014890818 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | -0.0309     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.817       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0397     |\n",
      "|    value_loss           | 4.93        |\n",
      "-----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.6e+03     |\n",
      "|    ep_rew_mean          | -3.67e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 736         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010656013 |\n",
      "|    clip_fraction        | 0.098       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.7        |\n",
      "|    explained_variance   | -0.0735     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.27        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    value_loss           | 11.7        |\n",
      "-----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=70000, episode_reward=-5654.60 +/- 566.19\n",
      "Episode length: 11511.60 +/- 1144.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.15e+04    |\n",
      "|    mean_reward          | -5.65e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018963877 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.62       |\n",
      "|    explained_variance   | -0.0142     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.389       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    value_loss           | 5.94        |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| reward/            |           |\n",
      "|    player_1        | [-1.]     |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 3.6e+03   |\n",
      "|    ep_rew_mean     | -3.67e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 91        |\n",
      "|    iterations      | 35        |\n",
      "|    time_elapsed    | 779       |\n",
      "|    total_timesteps | 71680     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.6e+03     |\n",
      "|    ep_rew_mean          | -3.67e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 788         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012239435 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.62       |\n",
      "|    explained_variance   | -0.167      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.91        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    value_loss           | 12.6        |\n",
      "-----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=75000, episode_reward=-6181.40 +/- 1924.06\n",
      "Episode length: 12565.20 +/- 3812.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+04    |\n",
      "|    mean_reward          | -6.18e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 75000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017698683 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | -0.108      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.13        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    value_loss           | 4.57        |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| reward/            |           |\n",
      "|    player_1        | [-1.]     |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 3.6e+03   |\n",
      "|    ep_rew_mean     | -3.67e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 87        |\n",
      "|    iterations      | 37        |\n",
      "|    time_elapsed    | 866       |\n",
      "|    total_timesteps | 75776     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.6e+03     |\n",
      "|    ep_rew_mean          | -3.67e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 875         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015405773 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.46       |\n",
      "|    explained_variance   | -0.0781     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.708       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    value_loss           | 4.04        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.6e+03     |\n",
      "|    ep_rew_mean          | -3.67e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 883         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019313514 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.46       |\n",
      "|    explained_variance   | -0.3        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.594       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=80000, episode_reward=-5674.80 +/- 2485.62\n",
      "Episode length: 11515.80 +/- 4971.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.15e+04    |\n",
      "|    mean_reward          | -5.67e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014895338 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.39       |\n",
      "|    explained_variance   | -0.0482     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.5         |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0425     |\n",
      "|    value_loss           | 4.82        |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| reward/            |           |\n",
      "|    player_1        | [-1.]     |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 3.6e+03   |\n",
      "|    ep_rew_mean     | -3.67e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 88        |\n",
      "|    iterations      | 40        |\n",
      "|    time_elapsed    | 928       |\n",
      "|    total_timesteps | 81920     |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| reward/                 |            |\n",
      "|    player_1             | [0.]       |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.6e+03    |\n",
      "|    ep_rew_mean          | -3.67e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 90         |\n",
      "|    iterations           | 41         |\n",
      "|    time_elapsed         | 931        |\n",
      "|    total_timesteps      | 83968      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01507161 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.37      |\n",
      "|    explained_variance   | -0.0304    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.287      |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0371    |\n",
      "|    value_loss           | 3.87       |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=85000, episode_reward=-5784.00 +/- 1345.04\n",
      "Episode length: 11801.20 +/- 2678.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.18e+04    |\n",
      "|    mean_reward          | -5.78e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 85000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025999203 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.3        |\n",
      "|    explained_variance   | -0.297      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0241      |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0497     |\n",
      "|    value_loss           | 0.826       |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| reward/            |           |\n",
      "|    player_1        | [0.]      |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 3.6e+03   |\n",
      "|    ep_rew_mean     | -3.67e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 85        |\n",
      "|    iterations      | 42        |\n",
      "|    time_elapsed    | 1007      |\n",
      "|    total_timesteps | 86016     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.6e+03     |\n",
      "|    ep_rew_mean          | -3.67e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 1015        |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017579181 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.37       |\n",
      "|    explained_variance   | -0.00217    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0908      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    value_loss           | 1.93        |\n",
      "-----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=90000, episode_reward=-7029.80 +/- 1232.48\n",
      "Episode length: 14288.40 +/- 2481.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.43e+04    |\n",
      "|    mean_reward          | -7.03e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 90000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011637259 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.31       |\n",
      "|    explained_variance   | -0.0303     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.496       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    value_loss           | 4.47        |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| reward/            |           |\n",
      "|    player_1        | [0.]      |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 3.6e+03   |\n",
      "|    ep_rew_mean     | -3.67e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 82        |\n",
      "|    iterations      | 44        |\n",
      "|    time_elapsed    | 1097      |\n",
      "|    total_timesteps | 90112     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.6e+03     |\n",
      "|    ep_rew_mean          | -3.67e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 1101        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016135812 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.28       |\n",
      "|    explained_variance   | -0.153      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.441       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0415     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.6e+03     |\n",
      "|    ep_rew_mean          | -3.67e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 1107        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015170335 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.29       |\n",
      "|    explained_variance   | 0.0246      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.149       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0353     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=95000, episode_reward=-7168.40 +/- 3164.33\n",
      "Episode length: 14488.40 +/- 6384.83\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.45e+04   |\n",
      "|    mean_reward          | -7.17e+03  |\n",
      "| reward/                 |            |\n",
      "|    player_1             | [0.]       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 95000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01718083 |\n",
      "|    clip_fraction        | 0.146      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.32      |\n",
      "|    explained_variance   | -0.118     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.67       |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0371    |\n",
      "|    value_loss           | 3.22       |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| reward/            |           |\n",
      "|    player_1        | [0.]      |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 3.6e+03   |\n",
      "|    ep_rew_mean     | -3.67e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 81        |\n",
      "|    iterations      | 47        |\n",
      "|    time_elapsed    | 1185      |\n",
      "|    total_timesteps | 96256     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.6e+03     |\n",
      "|    ep_rew_mean          | -3.67e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 1193        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020391993 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.24       |\n",
      "|    explained_variance   | -0.157      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.212       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0436     |\n",
      "|    value_loss           | 1.33        |\n",
      "-----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=100000, episode_reward=-8081.00 +/- 1951.86\n",
      "Episode length: 16083.00 +/- 3946.16\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.61e+04  |\n",
      "|    mean_reward          | -8.08e+03 |\n",
      "| reward/                 |           |\n",
      "|    player_1             | [-1.]     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 100000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0164476 |\n",
      "|    clip_fraction        | 0.196     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.31     |\n",
      "|    explained_variance   | 0.0575    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.354     |\n",
      "|    n_updates            | 480       |\n",
      "|    policy_gradient_loss | -0.0354   |\n",
      "|    value_loss           | 1.72      |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| reward/            |           |\n",
      "|    player_1        | [0.]      |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 3.6e+03   |\n",
      "|    ep_rew_mean     | -3.67e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 81        |\n",
      "|    iterations      | 49        |\n",
      "|    time_elapsed    | 1229      |\n",
      "|    total_timesteps | 100352    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x20876eb43e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_logging_callback = RewardLoggingCallback()\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_dir)\n",
    "model.learn(total_timesteps=100000, callback=[eval_callback, reward_logging_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info, _ = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.save(\"labyrinth_agent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
