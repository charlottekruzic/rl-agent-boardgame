{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook permettant d'entrainer nos agents RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from gym_env_2dim import LabyrinthEnv, RewardLoggingCallback, SaveModelCallback,ExplorationExploitationCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = LabyrinthEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace des actions :  MultiDiscrete([ 4 12])\n",
      "Espace des observations :  Box(0.0, 1.0, (245,), float32)\n"
     ]
    }
   ],
   "source": [
    "# Affichage des actions possibles et de l'espace d'observation\n",
    "print(\"Espace des actions : \", env.action_space)\n",
    "print(\"Espace des observations : \", env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./logs/\"\n",
    "env = Monitor(LabyrinthEnv(), log_dir)\n",
    "\n",
    "# Callback d'évaluation du modèle\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=2500,\n",
    "                             deterministic=True, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./logs/PPO_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\charl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.joueur_actuel to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.joueur_actuel` for environment variables or `env.get_wrapper_attr('joueur_actuel')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 1113     |\n",
      "|    positive_rewards       | 5        |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| time/                     |          |\n",
      "|    fps                    | 802      |\n",
      "|    iterations             | 1        |\n",
      "|    time_elapsed           | 2        |\n",
      "|    total_timesteps        | 2048     |\n",
      "| time_per_step             | 0        |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=2500, episode_reward=-10784.80 +/- 5284.76\n",
      "Episode length: 8532.80 +/- 3715.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 8.53e+03     |\n",
      "|    mean_reward          | -1.08e+04    |\n",
      "| reward/                 |              |\n",
      "|    player_1             | [0.]         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2500         |\n",
      "| time_per_step           | 0.00113      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068069315 |\n",
      "|    clip_fraction        | 0.0668       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.87        |\n",
      "|    explained_variance   | 0.00646      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 322          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0173      |\n",
      "|    value_loss           | 745          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\charl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:284: UserWarning: Path 'modeles\\agent_2' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 2281     |\n",
      "|    positive_rewards       | 13       |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [0.]     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 47       |\n",
      "|    iterations             | 2        |\n",
      "|    time_elapsed           | 86       |\n",
      "|    total_timesteps        | 4096     |\n",
      "| time_per_step             | 0        |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=5000, episode_reward=-9071.60 +/- 2682.46\n",
      "Episode length: 17910.80 +/- 5222.37\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.79e+04   |\n",
      "|    mean_reward          | -9.07e+03  |\n",
      "| reward/                 |            |\n",
      "|    player_1             | [-10.]     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5000       |\n",
      "| time_per_step           | 0.00746    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00922921 |\n",
      "|    clip_fraction        | 0.0886     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.85      |\n",
      "|    explained_variance   | -6.29e-05  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 551        |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0204    |\n",
      "|    value_loss           | 1.3e+03    |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 3372     |\n",
      "|    positive_rewards       | 18       |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| time/                     |          |\n",
      "|    fps                    | 33       |\n",
      "|    iterations             | 3        |\n",
      "|    time_elapsed           | 182      |\n",
      "|    total_timesteps        | 6144     |\n",
      "| time_per_step             | 0        |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=7500, episode_reward=-5486.80 +/- 3120.57\n",
      "Episode length: 10512.40 +/- 6038.50\n",
      "-------------------------------------------\n",
      "| episode_length            | 1923        |\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.05e+04    |\n",
      "|    mean_reward            | -5.49e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 3892        |\n",
      "|    positive_rewards       | 22          |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [-10.]      |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 7500        |\n",
      "| time_per_step             | 0           |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.008403568 |\n",
      "|    clip_fraction          | 0.089       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -3.83       |\n",
      "|    explained_variance     | 1.16e-05    |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 116         |\n",
      "|    n_updates              | 30          |\n",
      "|    policy_gradient_loss   | -0.0265     |\n",
      "|    value_loss             | 433         |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| exploration_exploitation/ |           |\n",
      "|    negative_rewards       | 4431      |\n",
      "|    positive_rewards       | 27        |\n",
      "| reward/                   |           |\n",
      "|    player_1               | [-10.]    |\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 1.92e+03  |\n",
      "|    ep_rew_mean            | -2.12e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 36        |\n",
      "|    iterations             | 4         |\n",
      "|    time_elapsed           | 225       |\n",
      "|    total_timesteps        | 8192      |\n",
      "| time_per_step             | 0         |\n",
      "-----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=10000, episode_reward=-7890.00 +/- 3250.62\n",
      "Episode length: 15187.20 +/- 5478.80\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.52e+04    |\n",
      "|    mean_reward            | -7.89e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 4988        |\n",
      "|    positive_rewards       | 31          |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [0.]        |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 10000       |\n",
      "| time_per_step             | 0           |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.008095212 |\n",
      "|    clip_fraction          | 0.09        |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -3.81       |\n",
      "|    explained_variance     | -4.05e-06   |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 283         |\n",
      "|    n_updates              | 40          |\n",
      "|    policy_gradient_loss   | -0.0264     |\n",
      "|    value_loss             | 397         |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| exploration_exploitation/ |           |\n",
      "|    negative_rewards       | 5534      |\n",
      "|    positive_rewards       | 32        |\n",
      "| reward/                   |           |\n",
      "|    player_1               | [-1.]     |\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 1.92e+03  |\n",
      "|    ep_rew_mean            | -2.12e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 31        |\n",
      "|    iterations             | 5         |\n",
      "|    time_elapsed           | 325       |\n",
      "|    total_timesteps        | 10240     |\n",
      "| time_per_step             | 0.00798   |\n",
      "-----------------------------------------\n",
      "--------------------------------------------\n",
      "| exploration_exploitation/ |              |\n",
      "|    negative_rewards       | 6610         |\n",
      "|    positive_rewards       | 38           |\n",
      "| reward/                   |              |\n",
      "|    player_1               | [-1.]        |\n",
      "| rollout/                  |              |\n",
      "|    ep_len_mean            | 1.92e+03     |\n",
      "|    ep_rew_mean            | -2.12e+03    |\n",
      "| time/                     |              |\n",
      "|    fps                    | 36           |\n",
      "|    iterations             | 6            |\n",
      "|    time_elapsed           | 337          |\n",
      "|    total_timesteps        | 12288        |\n",
      "| time_per_step             | 0            |\n",
      "| train/                    |              |\n",
      "|    approx_kl              | 0.0096091535 |\n",
      "|    clip_fraction          | 0.0991       |\n",
      "|    clip_range             | 0.2          |\n",
      "|    entropy_loss           | -3.77        |\n",
      "|    explained_variance     | -5.6e-06     |\n",
      "|    learning_rate          | 0.0003       |\n",
      "|    loss                   | 125          |\n",
      "|    n_updates              | 50           |\n",
      "|    policy_gradient_loss   | -0.0303      |\n",
      "|    value_loss             | 479          |\n",
      "--------------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=12500, episode_reward=-9303.40 +/- 2273.94\n",
      "Episode length: 15475.80 +/- 3704.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.55e+04    |\n",
      "|    mean_reward          | -9.3e+03    |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| time_per_step           | 0           |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011840658 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.74       |\n",
      "|    explained_variance   | 4.59e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 132         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0364     |\n",
      "|    value_loss           | 241         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| exploration_exploitation/ |           |\n",
      "|    negative_rewards       | 7681      |\n",
      "|    positive_rewards       | 46        |\n",
      "| reward/                   |           |\n",
      "|    player_1               | [-1.]     |\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 1.92e+03  |\n",
      "|    ep_rew_mean            | -2.12e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 35        |\n",
      "|    iterations             | 7         |\n",
      "|    time_elapsed           | 401       |\n",
      "|    total_timesteps        | 14336     |\n",
      "| time_per_step             | 0.00761   |\n",
      "-----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=15000, episode_reward=-8666.40 +/- 2261.96\n",
      "Episode length: 12298.60 +/- 1199.88\n",
      "-----------------------------------------\n",
      "| episode_length          | 2120        |\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.23e+04    |\n",
      "|    mean_reward          | -8.67e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| time_per_step           | 0           |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010997811 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.71       |\n",
      "|    explained_variance   | -4.05e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 117         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0376     |\n",
      "|    value_loss           | 236         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| exploration_exploitation/ |           |\n",
      "|    negative_rewards       | 8742      |\n",
      "|    positive_rewards       | 55        |\n",
      "| reward/                   |           |\n",
      "|    player_1               | [-10.]    |\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 2.02e+03  |\n",
      "|    ep_rew_mean            | -2.34e+03 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 35        |\n",
      "|    iterations             | 8         |\n",
      "|    time_elapsed           | 455       |\n",
      "|    total_timesteps        | 16384     |\n",
      "| time_per_step             | 0.00819   |\n",
      "-----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=17500, episode_reward=-10043.00 +/- 3232.19\n",
      "Episode length: 11808.00 +/- 3611.02\n",
      "-------------------------------------------\n",
      "| episode_length            | 1463        |\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.18e+04    |\n",
      "|    mean_reward            | -1e+04      |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 9295        |\n",
      "|    positive_rewards       | 56          |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [0.]        |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 17500       |\n",
      "| time_per_step             | 0.00127     |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.013164867 |\n",
      "|    clip_fraction          | 0.157       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -3.72       |\n",
      "|    explained_variance     | 3.04e-06    |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 132         |\n",
      "|    n_updates              | 80          |\n",
      "|    policy_gradient_loss   | -0.0393     |\n",
      "|    value_loss             | 324         |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 9825     |\n",
      "|    positive_rewards       | 59       |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 36       |\n",
      "|    iterations             | 9        |\n",
      "|    time_elapsed           | 509      |\n",
      "|    total_timesteps        | 18432    |\n",
      "| time_per_step             | 0.0018   |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=20000, episode_reward=-9847.20 +/- 4165.58\n",
      "Episode length: 13605.60 +/- 4838.82\n",
      "------------------------------------------\n",
      "| eval/                     |            |\n",
      "|    mean_ep_length         | 1.36e+04   |\n",
      "|    mean_reward            | -9.85e+03  |\n",
      "| exploration_exploitation/ |            |\n",
      "|    negative_rewards       | 10387      |\n",
      "|    positive_rewards       | 60         |\n",
      "| reward/                   |            |\n",
      "|    player_1               | [-1.]      |\n",
      "| time/                     |            |\n",
      "|    total_timesteps        | 20000      |\n",
      "| time_per_step             | 0.00201    |\n",
      "| train/                    |            |\n",
      "|    approx_kl              | 0.02605027 |\n",
      "|    clip_fraction          | 0.237      |\n",
      "|    clip_range             | 0.2        |\n",
      "|    entropy_loss           | -3.7       |\n",
      "|    explained_variance     | 1.01e-06   |\n",
      "|    learning_rate          | 0.0003     |\n",
      "|    loss                   | 92.1       |\n",
      "|    n_updates              | 90         |\n",
      "|    policy_gradient_loss   | -0.0424    |\n",
      "|    value_loss             | 161        |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 10914    |\n",
      "|    positive_rewards       | 64       |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [0.]     |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 34       |\n",
      "|    iterations             | 10       |\n",
      "|    time_elapsed           | 592      |\n",
      "|    total_timesteps        | 20480    |\n",
      "| time_per_step             | 0        |\n",
      "----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=22500, episode_reward=-11804.00 +/- 7156.15\n",
      "Episode length: 16330.40 +/- 7798.82\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.63e+04    |\n",
      "|    mean_reward            | -1.18e+04   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 11988       |\n",
      "|    positive_rewards       | 67          |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [-1.]       |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 22500       |\n",
      "| time_per_step             | 0.00277     |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.008481477 |\n",
      "|    clip_fraction          | 0.0732      |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -3.64       |\n",
      "|    explained_variance     | 7.15e-07    |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 117         |\n",
      "|    n_updates              | 100         |\n",
      "|    policy_gradient_loss   | -0.0257     |\n",
      "|    value_loss             | 349         |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| reward/            |          |\n",
      "|    player_1        | [-1.]    |\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.84e+03 |\n",
      "|    ep_rew_mean     | -2.2e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 33       |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 679      |\n",
      "|    total_timesteps | 22528    |\n",
      "| time_per_step      | 0.0058   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 13060       |\n",
      "|    positive_rewards       | 73          |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [-1.]       |\n",
      "| rollout/                  |             |\n",
      "|    ep_len_mean            | 1.84e+03    |\n",
      "|    ep_rew_mean            | -2.2e+03    |\n",
      "| time/                     |             |\n",
      "|    fps                    | 35          |\n",
      "|    iterations             | 12          |\n",
      "|    time_elapsed           | 688         |\n",
      "|    total_timesteps        | 24576       |\n",
      "| time_per_step             | 0.00469     |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.021502908 |\n",
      "|    clip_fraction          | 0.122       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -3.6        |\n",
      "|    explained_variance     | -9.54e-07   |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 169         |\n",
      "|    n_updates              | 110         |\n",
      "|    policy_gradient_loss   | -0.0293     |\n",
      "|    value_loss             | 276         |\n",
      "-------------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=25000, episode_reward=-8712.80 +/- 3575.49\n",
      "Episode length: 12216.00 +/- 5498.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.22e+04    |\n",
      "|    mean_reward          | -8.71e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| time_per_step           | 0           |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021030502 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.57       |\n",
      "|    explained_variance   | -2.38e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 109         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    value_loss           | 205         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 14103    |\n",
      "|    positive_rewards       | 74       |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [0.]     |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 33       |\n",
      "|    iterations             | 13       |\n",
      "|    time_elapsed           | 784      |\n",
      "|    total_timesteps        | 26624    |\n",
      "| time_per_step             | 0        |\n",
      "----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=27500, episode_reward=-7956.00 +/- 2072.50\n",
      "Episode length: 13732.20 +/- 4052.13\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.37e+04    |\n",
      "|    mean_reward            | -7.96e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 14624       |\n",
      "|    positive_rewards       | 79          |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [-1.]       |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 27500       |\n",
      "| time_per_step             | 0.00464     |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.022034118 |\n",
      "|    clip_fraction          | 0.308       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -3.59       |\n",
      "|    explained_variance     | 0           |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 60.8        |\n",
      "|    n_updates              | 130         |\n",
      "|    policy_gradient_loss   | -0.0443     |\n",
      "|    value_loss             | 93.9        |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 15146    |\n",
      "|    positive_rewards       | 81       |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [0.]     |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 33       |\n",
      "|    iterations             | 14       |\n",
      "|    time_elapsed           | 858      |\n",
      "|    total_timesteps        | 28672    |\n",
      "| time_per_step             | 0        |\n",
      "----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=30000, episode_reward=-8660.20 +/- 3771.36\n",
      "Episode length: 13087.40 +/- 5056.52\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.31e+04    |\n",
      "|    mean_reward            | -8.66e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 15669       |\n",
      "|    positive_rewards       | 81          |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [0.]        |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 30000       |\n",
      "| time_per_step             | 0.00151     |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.017642166 |\n",
      "|    clip_fraction          | 0.173       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -3.52       |\n",
      "|    explained_variance     | -5.96e-07   |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 32.4        |\n",
      "|    n_updates              | 140         |\n",
      "|    policy_gradient_loss   | -0.0306     |\n",
      "|    value_loss             | 79.4        |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 16197    |\n",
      "|    positive_rewards       | 84       |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 33       |\n",
      "|    iterations             | 15       |\n",
      "|    time_elapsed           | 915      |\n",
      "|    total_timesteps        | 30720    |\n",
      "| time_per_step             | 0.00652  |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=32500, episode_reward=-6324.20 +/- 2414.50\n",
      "Episode length: 10978.00 +/- 4568.71\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.1e+04     |\n",
      "|    mean_reward            | -6.32e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 17236       |\n",
      "|    positive_rewards       | 91          |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [0.]        |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 32500       |\n",
      "| time_per_step             | 0           |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.017067196 |\n",
      "|    clip_fraction          | 0.219       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -3.49       |\n",
      "|    explained_variance     | 1.01e-06    |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 59.4        |\n",
      "|    n_updates              | 150         |\n",
      "|    policy_gradient_loss   | -0.0421     |\n",
      "|    value_loss             | 84.8        |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| reward/            |          |\n",
      "|    player_1        | [-1.]    |\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.84e+03 |\n",
      "|    ep_rew_mean     | -2.2e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 33       |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 964      |\n",
      "|    total_timesteps | 32768    |\n",
      "| time_per_step      | 0        |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 18283       |\n",
      "|    positive_rewards       | 96          |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [-1.]       |\n",
      "| rollout/                  |             |\n",
      "|    ep_len_mean            | 1.84e+03    |\n",
      "|    ep_rew_mean            | -2.2e+03    |\n",
      "| time/                     |             |\n",
      "|    fps                    | 35          |\n",
      "|    iterations             | 17          |\n",
      "|    time_elapsed           | 970         |\n",
      "|    total_timesteps        | 34816       |\n",
      "| time_per_step             | 0           |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.015848998 |\n",
      "|    clip_fraction          | 0.187       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -3.41       |\n",
      "|    explained_variance     | 7.15e-07    |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 45.5        |\n",
      "|    n_updates              | 160         |\n",
      "|    policy_gradient_loss   | -0.0418     |\n",
      "|    value_loss             | 65.6        |\n",
      "-------------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=35000, episode_reward=-7796.20 +/- 2781.76\n",
      "Episode length: 14760.40 +/- 4650.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.48e+04    |\n",
      "|    mean_reward          | -7.8e+03    |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35000       |\n",
      "| time_per_step           | 0.00162     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015681569 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.38       |\n",
      "|    explained_variance   | -1.79e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 49.6        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0348     |\n",
      "|    value_loss           | 79.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 19315    |\n",
      "|    positive_rewards       | 104      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 35       |\n",
      "|    iterations             | 18       |\n",
      "|    time_elapsed           | 1051     |\n",
      "|    total_timesteps        | 36864    |\n",
      "| time_per_step             | 0.00056  |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=37500, episode_reward=-8469.80 +/- 3370.64\n",
      "Episode length: 16252.60 +/- 6601.44\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.63e+04    |\n",
      "|    mean_reward            | -8.47e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 19823       |\n",
      "|    positive_rewards       | 105         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [0.]        |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 37500       |\n",
      "| time_per_step             | 0           |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.012851886 |\n",
      "|    clip_fraction          | 0.181       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -3.33       |\n",
      "|    explained_variance     | 5.96e-08    |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 19.6        |\n",
      "|    n_updates              | 180         |\n",
      "|    policy_gradient_loss   | -0.0335     |\n",
      "|    value_loss             | 63.4        |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 20337    |\n",
      "|    positive_rewards       | 109      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [0.]     |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 34       |\n",
      "|    iterations             | 19       |\n",
      "|    time_elapsed           | 1133     |\n",
      "|    total_timesteps        | 38912    |\n",
      "| time_per_step             | 0        |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=40000, episode_reward=-7864.20 +/- 1638.22\n",
      "Episode length: 15423.80 +/- 3432.63\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.54e+04    |\n",
      "|    mean_reward            | -7.86e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 20851       |\n",
      "|    positive_rewards       | 110         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [-1.]       |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 40000       |\n",
      "| time_per_step             | 0           |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.008805448 |\n",
      "|    clip_fraction          | 0.133       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -3.29       |\n",
      "|    explained_variance     | 1.31e-06    |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 25          |\n",
      "|    n_updates              | 190         |\n",
      "|    policy_gradient_loss   | -0.032      |\n",
      "|    value_loss             | 63          |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 21356    |\n",
      "|    positive_rewards       | 113      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 33       |\n",
      "|    iterations             | 20       |\n",
      "|    time_elapsed           | 1220     |\n",
      "|    total_timesteps        | 40960    |\n",
      "| time_per_step             | 0        |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=42500, episode_reward=-7470.60 +/- 2513.27\n",
      "Episode length: 14915.00 +/- 4894.96\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.49e+04    |\n",
      "|    mean_reward            | -7.47e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 22382       |\n",
      "|    positive_rewards       | 115         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [0.]        |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 42500       |\n",
      "| time_per_step             | 0           |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.007778882 |\n",
      "|    clip_fraction          | 0.0636      |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -3.26       |\n",
      "|    explained_variance     | 0.0366      |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 8.12        |\n",
      "|    n_updates              | 200         |\n",
      "|    policy_gradient_loss   | -0.0232     |\n",
      "|    value_loss             | 18.6        |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 22889    |\n",
      "|    positive_rewards       | 118      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 33       |\n",
      "|    iterations             | 21       |\n",
      "|    time_elapsed           | 1287     |\n",
      "|    total_timesteps        | 43008    |\n",
      "| time_per_step             | 0.00127  |\n",
      "----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=45000, episode_reward=-7839.20 +/- 1490.78\n",
      "Episode length: 15313.80 +/- 2953.16\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.53e+04    |\n",
      "|    mean_reward            | -7.84e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 23401       |\n",
      "|    positive_rewards       | 119         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [0.]        |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 45000       |\n",
      "| time_per_step             | 0           |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.009403632 |\n",
      "|    clip_fraction          | 0.0775      |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -3.23       |\n",
      "|    explained_variance     | -0.0364     |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 6.26        |\n",
      "|    n_updates              | 210         |\n",
      "|    policy_gradient_loss   | -0.0261     |\n",
      "|    value_loss             | 17.9        |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 23910    |\n",
      "|    positive_rewards       | 122      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 32       |\n",
      "|    iterations             | 22       |\n",
      "|    time_elapsed           | 1376     |\n",
      "|    total_timesteps        | 45056    |\n",
      "| time_per_step             | 0        |\n",
      "----------------------------------------\n",
      "--------------------------------------------\n",
      "| exploration_exploitation/ |              |\n",
      "|    negative_rewards       | 24931        |\n",
      "|    positive_rewards       | 129          |\n",
      "| reward/                   |              |\n",
      "|    player_1               | [-1.]        |\n",
      "| rollout/                  |              |\n",
      "|    ep_len_mean            | 1.84e+03     |\n",
      "|    ep_rew_mean            | -2.2e+03     |\n",
      "| time/                     |              |\n",
      "|    fps                    | 34           |\n",
      "|    iterations             | 23           |\n",
      "|    time_elapsed           | 1381         |\n",
      "|    total_timesteps        | 47104        |\n",
      "| time_per_step             | 0            |\n",
      "| train/                    |              |\n",
      "|    approx_kl              | 0.0057576383 |\n",
      "|    clip_fraction          | 0.0605       |\n",
      "|    clip_range             | 0.2          |\n",
      "|    entropy_loss           | -3.2         |\n",
      "|    explained_variance     | -0.309       |\n",
      "|    learning_rate          | 0.0003       |\n",
      "|    loss                   | 6.58         |\n",
      "|    n_updates              | 220          |\n",
      "|    policy_gradient_loss   | -0.0214      |\n",
      "|    value_loss             | 16.1         |\n",
      "--------------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=47500, episode_reward=-9795.60 +/- 3869.02\n",
      "Episode length: 19397.60 +/- 7324.37\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.94e+04     |\n",
      "|    mean_reward          | -9.8e+03     |\n",
      "| reward/                 |              |\n",
      "|    player_1             | [-1.]        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 47500        |\n",
      "| time_per_step           | 0.00417      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076159365 |\n",
      "|    clip_fraction        | 0.0444       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.18        |\n",
      "|    explained_variance   | -0.107       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.73         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.0184      |\n",
      "|    value_loss           | 21.8         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 25959    |\n",
      "|    positive_rewards       | 135      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 33       |\n",
      "|    iterations             | 24       |\n",
      "|    time_elapsed           | 1457     |\n",
      "|    total_timesteps        | 49152    |\n",
      "| time_per_step             | 0        |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=50000, episode_reward=-8202.80 +/- 2096.33\n",
      "Episode length: 16625.60 +/- 4175.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.66e+04    |\n",
      "|    mean_reward          | -8.2e+03    |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| time_per_step           | 0           |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006739668 |\n",
      "|    clip_fraction        | 0.0559      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.2        |\n",
      "|    explained_variance   | -0.33       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.2        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 25.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 26979    |\n",
      "|    positive_rewards       | 138      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [0.]     |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 33       |\n",
      "|    iterations             | 25       |\n",
      "|    time_elapsed           | 1536     |\n",
      "|    total_timesteps        | 51200    |\n",
      "| time_per_step             | 0.00248  |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=52500, episode_reward=-7037.40 +/- 2081.85\n",
      "Episode length: 14261.60 +/- 4211.36\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.43e+04    |\n",
      "|    mean_reward            | -7.04e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 27485       |\n",
      "|    positive_rewards       | 140         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [-1.]       |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 52500       |\n",
      "| time_per_step             | 0           |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.007291983 |\n",
      "|    clip_fraction          | 0.0511      |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -3.09       |\n",
      "|    explained_variance     | 0.0269      |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 8.81        |\n",
      "|    n_updates              | 250         |\n",
      "|    policy_gradient_loss   | -0.0189     |\n",
      "|    value_loss             | 22.2        |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 27999    |\n",
      "|    positive_rewards       | 141      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [0.]     |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 32       |\n",
      "|    iterations             | 26       |\n",
      "|    time_elapsed           | 1618     |\n",
      "|    total_timesteps        | 53248    |\n",
      "| time_per_step             | 0.00263  |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=55000, episode_reward=-6943.80 +/- 1271.78\n",
      "Episode length: 14065.80 +/- 2598.00\n",
      "--------------------------------------------\n",
      "| eval/                     |              |\n",
      "|    mean_ep_length         | 1.41e+04     |\n",
      "|    mean_reward            | -6.94e+03    |\n",
      "| exploration_exploitation/ |              |\n",
      "|    negative_rewards       | 28503        |\n",
      "|    positive_rewards       | 143          |\n",
      "| reward/                   |              |\n",
      "|    player_1               | [-10.]       |\n",
      "| time/                     |              |\n",
      "|    total_timesteps        | 55000        |\n",
      "| time_per_step             | 0            |\n",
      "| train/                    |              |\n",
      "|    approx_kl              | 0.0069755865 |\n",
      "|    clip_fraction          | 0.0438       |\n",
      "|    clip_range             | 0.2          |\n",
      "|    entropy_loss           | -3.11        |\n",
      "|    explained_variance     | -0.284       |\n",
      "|    learning_rate          | 0.0003       |\n",
      "|    loss                   | 5.89         |\n",
      "|    n_updates              | 260          |\n",
      "|    policy_gradient_loss   | -0.0229      |\n",
      "|    value_loss             | 20.7         |\n",
      "--------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 29014    |\n",
      "|    positive_rewards       | 145      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [0.]     |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 33       |\n",
      "|    iterations             | 27       |\n",
      "|    time_elapsed           | 1671     |\n",
      "|    total_timesteps        | 55296    |\n",
      "| time_per_step             | 0.000558 |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| exploration_exploitation/ |            |\n",
      "|    negative_rewards       | 30027      |\n",
      "|    positive_rewards       | 150        |\n",
      "| reward/                   |            |\n",
      "|    player_1               | [-1.]      |\n",
      "| rollout/                  |            |\n",
      "|    ep_len_mean            | 1.84e+03   |\n",
      "|    ep_rew_mean            | -2.2e+03   |\n",
      "| time/                     |            |\n",
      "|    fps                    | 34         |\n",
      "|    iterations             | 28         |\n",
      "|    time_elapsed           | 1675       |\n",
      "|    total_timesteps        | 57344      |\n",
      "| time_per_step             | 0          |\n",
      "| train/                    |            |\n",
      "|    approx_kl              | 0.00920652 |\n",
      "|    clip_fraction          | 0.0904     |\n",
      "|    clip_range             | 0.2        |\n",
      "|    entropy_loss           | -3.03      |\n",
      "|    explained_variance     | 0.0936     |\n",
      "|    learning_rate          | 0.0003     |\n",
      "|    loss                   | 2.54       |\n",
      "|    n_updates              | 270        |\n",
      "|    policy_gradient_loss   | -0.0301    |\n",
      "|    value_loss             | 11.5       |\n",
      "------------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=57500, episode_reward=-7595.40 +/- 2862.45\n",
      "Episode length: 15363.20 +/- 5672.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.54e+04    |\n",
      "|    mean_reward          | -7.6e+03    |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 57500       |\n",
      "| time_per_step           | 0.000997    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009990603 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.02       |\n",
      "|    explained_variance   | -0.168      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.37        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0324     |\n",
      "|    value_loss           | 16          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 31036    |\n",
      "|    positive_rewards       | 153      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [0.]     |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 34       |\n",
      "|    iterations             | 29       |\n",
      "|    time_elapsed           | 1707     |\n",
      "|    total_timesteps        | 59392    |\n",
      "| time_per_step             | 0.00101  |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=60000, episode_reward=-8331.40 +/- 2624.44\n",
      "Episode length: 16892.20 +/- 5277.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.69e+04    |\n",
      "|    mean_reward          | -8.33e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| time_per_step           | 0.000991    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012557924 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.98       |\n",
      "|    explained_variance   | -0.327      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.61        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.035      |\n",
      "|    value_loss           | 8.71        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 32046    |\n",
      "|    positive_rewards       | 159      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 35       |\n",
      "|    iterations             | 30       |\n",
      "|    time_elapsed           | 1740     |\n",
      "|    total_timesteps        | 61440    |\n",
      "| time_per_step             | 0        |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=62500, episode_reward=-6909.20 +/- 1244.90\n",
      "Episode length: 14034.00 +/- 2482.35\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.4e+04     |\n",
      "|    mean_reward            | -6.91e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 32556       |\n",
      "|    positive_rewards       | 160         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [0.]        |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 62500       |\n",
      "| time_per_step             | 0           |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.010512864 |\n",
      "|    clip_fraction          | 0.111       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -2.93       |\n",
      "|    explained_variance     | -0.0191     |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 2.98        |\n",
      "|    n_updates              | 300         |\n",
      "|    policy_gradient_loss   | -0.0351     |\n",
      "|    value_loss             | 12.7        |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 33060    |\n",
      "|    positive_rewards       | 162      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 35       |\n",
      "|    iterations             | 31       |\n",
      "|    time_elapsed           | 1771     |\n",
      "|    total_timesteps        | 63488    |\n",
      "| time_per_step             | 0.000977 |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=65000, episode_reward=-9129.80 +/- 1190.43\n",
      "Episode length: 18304.00 +/- 2241.07\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.83e+04    |\n",
      "|    mean_reward            | -9.13e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 33565       |\n",
      "|    positive_rewards       | 165         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [-1.]       |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 65000       |\n",
      "| time_per_step             | 0           |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.011940705 |\n",
      "|    clip_fraction          | 0.105       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -2.91       |\n",
      "|    explained_variance     | -0.0683     |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 3.7         |\n",
      "|    n_updates              | 310         |\n",
      "|    policy_gradient_loss   | -0.0376     |\n",
      "|    value_loss             | 10.1        |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 34069    |\n",
      "|    positive_rewards       | 167      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [0.]     |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 36       |\n",
      "|    iterations             | 32       |\n",
      "|    time_elapsed           | 1810     |\n",
      "|    total_timesteps        | 65536    |\n",
      "| time_per_step             | 0.00151  |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=67500, episode_reward=-9745.80 +/- 4395.94\n",
      "Episode length: 19738.00 +/- 8799.16\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.97e+04    |\n",
      "|    mean_reward            | -9.75e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 35087       |\n",
      "|    positive_rewards       | 168         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [0.]        |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 67500       |\n",
      "| time_per_step             | 0.000997    |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.010737514 |\n",
      "|    clip_fraction          | 0.116       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -2.86       |\n",
      "|    explained_variance     | 0.0522      |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 3.07        |\n",
      "|    n_updates              | 320         |\n",
      "|    policy_gradient_loss   | -0.0319     |\n",
      "|    value_loss             | 10.8        |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| reward/            |          |\n",
      "|    player_1        | [-1.]    |\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.84e+03 |\n",
      "|    ep_rew_mean     | -2.2e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 1851     |\n",
      "|    total_timesteps | 67584    |\n",
      "| time_per_step      | 0.000999 |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 36091       |\n",
      "|    positive_rewards       | 172         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [-1.]       |\n",
      "| rollout/                  |             |\n",
      "|    ep_len_mean            | 1.84e+03    |\n",
      "|    ep_rew_mean            | -2.2e+03    |\n",
      "| time/                     |             |\n",
      "|    fps                    | 37          |\n",
      "|    iterations             | 34          |\n",
      "|    time_elapsed           | 1854        |\n",
      "|    total_timesteps        | 69632       |\n",
      "| time_per_step             | 0.00103     |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.014341708 |\n",
      "|    clip_fraction          | 0.137       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -2.82       |\n",
      "|    explained_variance     | -6.4e-05    |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 1.02        |\n",
      "|    n_updates              | 330         |\n",
      "|    policy_gradient_loss   | -0.0415     |\n",
      "|    value_loss             | 8.76        |\n",
      "-------------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=70000, episode_reward=-8001.80 +/- 3263.33\n",
      "Episode length: 16229.20 +/- 6556.36\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.62e+04   |\n",
      "|    mean_reward          | -8e+03     |\n",
      "| reward/                 |            |\n",
      "|    player_1             | [0.]       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 70000      |\n",
      "| time_per_step           | 0.000898   |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01764181 |\n",
      "|    clip_fraction        | 0.194      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.81      |\n",
      "|    explained_variance   | -0.0784    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.926      |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0491    |\n",
      "|    value_loss           | 5.19       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 37097    |\n",
      "|    positive_rewards       | 175      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [0.]     |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 37       |\n",
      "|    iterations             | 35       |\n",
      "|    time_elapsed           | 1887     |\n",
      "|    total_timesteps        | 71680    |\n",
      "| time_per_step             | 0.001    |\n",
      "----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=72500, episode_reward=-7880.20 +/- 2584.06\n",
      "Episode length: 15183.00 +/- 4705.75\n",
      "------------------------------------------\n",
      "| eval/                     |            |\n",
      "|    mean_ep_length         | 1.52e+04   |\n",
      "|    mean_reward            | -7.88e+03  |\n",
      "| exploration_exploitation/ |            |\n",
      "|    negative_rewards       | 37601      |\n",
      "|    positive_rewards       | 176        |\n",
      "| reward/                   |            |\n",
      "|    player_1               | [0.]       |\n",
      "| time/                     |            |\n",
      "|    total_timesteps        | 72500      |\n",
      "| time_per_step             | 0          |\n",
      "| train/                    |            |\n",
      "|    approx_kl              | 0.01281792 |\n",
      "|    clip_fraction          | 0.126      |\n",
      "|    clip_range             | 0.2        |\n",
      "|    entropy_loss           | -2.72      |\n",
      "|    explained_variance     | -0.0411    |\n",
      "|    learning_rate          | 0.0003     |\n",
      "|    loss                   | 5.25       |\n",
      "|    n_updates              | 350        |\n",
      "|    policy_gradient_loss   | -0.032     |\n",
      "|    value_loss             | 10.1       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 38108    |\n",
      "|    positive_rewards       | 178      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 38       |\n",
      "|    iterations             | 36       |\n",
      "|    time_elapsed           | 1918     |\n",
      "|    total_timesteps        | 73728    |\n",
      "| time_per_step             | 0.00101  |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=75000, episode_reward=-9103.20 +/- 4523.51\n",
      "Episode length: 18324.20 +/- 8779.40\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.83e+04    |\n",
      "|    mean_reward            | -9.1e+03    |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 38613       |\n",
      "|    positive_rewards       | 178         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [0.]        |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 75000       |\n",
      "| time_per_step             | 0           |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.013231758 |\n",
      "|    clip_fraction          | 0.134       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -2.72       |\n",
      "|    explained_variance     | 0.283       |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 2.21        |\n",
      "|    n_updates              | 360         |\n",
      "|    policy_gradient_loss   | -0.0361     |\n",
      "|    value_loss             | 8.9         |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 39115    |\n",
      "|    positive_rewards       | 180      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 38       |\n",
      "|    iterations             | 37       |\n",
      "|    time_elapsed           | 1957     |\n",
      "|    total_timesteps        | 75776    |\n",
      "| time_per_step             | 0.00105  |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=77500, episode_reward=-9280.20 +/- 3679.29\n",
      "Episode length: 18793.60 +/- 7380.42\n",
      "------------------------------------------\n",
      "| eval/                     |            |\n",
      "|    mean_ep_length         | 1.88e+04   |\n",
      "|    mean_reward            | -9.28e+03  |\n",
      "| exploration_exploitation/ |            |\n",
      "|    negative_rewards       | 40120      |\n",
      "|    positive_rewards       | 183        |\n",
      "| reward/                   |            |\n",
      "|    player_1               | [0.]       |\n",
      "| time/                     |            |\n",
      "|    total_timesteps        | 77500      |\n",
      "| time_per_step             | 0.00131    |\n",
      "| train/                    |            |\n",
      "|    approx_kl              | 0.01825166 |\n",
      "|    clip_fraction          | 0.196      |\n",
      "|    clip_range             | 0.2        |\n",
      "|    entropy_loss           | -2.66      |\n",
      "|    explained_variance     | -0.15      |\n",
      "|    learning_rate          | 0.0003     |\n",
      "|    loss                   | 2.14       |\n",
      "|    n_updates              | 370        |\n",
      "|    policy_gradient_loss   | -0.0468    |\n",
      "|    value_loss             | 7.18       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| reward/            |          |\n",
      "|    player_1        | [-1.]    |\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.84e+03 |\n",
      "|    ep_rew_mean     | -2.2e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 1997     |\n",
      "|    total_timesteps | 77824    |\n",
      "| time_per_step      | 0.000626 |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 41131       |\n",
      "|    positive_rewards       | 185         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [0.]        |\n",
      "| rollout/                  |             |\n",
      "|    ep_len_mean            | 1.84e+03    |\n",
      "|    ep_rew_mean            | -2.2e+03    |\n",
      "| time/                     |             |\n",
      "|    fps                    | 39          |\n",
      "|    iterations             | 39          |\n",
      "|    time_elapsed           | 2001        |\n",
      "|    total_timesteps        | 79872       |\n",
      "| time_per_step             | 0           |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.019532908 |\n",
      "|    clip_fraction          | 0.215       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -2.65       |\n",
      "|    explained_variance     | -0.12       |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 1.41        |\n",
      "|    n_updates              | 380         |\n",
      "|    policy_gradient_loss   | -0.0529     |\n",
      "|    value_loss             | 6.24        |\n",
      "-------------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=80000, episode_reward=-8175.20 +/- 3119.11\n",
      "Episode length: 15909.80 +/- 5618.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.59e+04    |\n",
      "|    mean_reward          | -8.18e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| time_per_step           | 0.000507    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016729193 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.66       |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.93        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0392     |\n",
      "|    value_loss           | 9.83        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 42134    |\n",
      "|    positive_rewards       | 189      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 40       |\n",
      "|    iterations             | 40       |\n",
      "|    time_elapsed           | 2033     |\n",
      "|    total_timesteps        | 81920    |\n",
      "| time_per_step             | 0.00194  |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=82500, episode_reward=-10026.60 +/- 3779.84\n",
      "Episode length: 20021.00 +/- 7538.94\n",
      "------------------------------------------\n",
      "| eval/                     |            |\n",
      "|    mean_ep_length         | 2e+04      |\n",
      "|    mean_reward            | -1e+04     |\n",
      "| exploration_exploitation/ |            |\n",
      "|    negative_rewards       | 42635      |\n",
      "|    positive_rewards       | 190        |\n",
      "| reward/                   |            |\n",
      "|    player_1               | [-1.]      |\n",
      "| time/                     |            |\n",
      "|    total_timesteps        | 82500      |\n",
      "| time_per_step             | 0          |\n",
      "| train/                    |            |\n",
      "|    approx_kl              | 0.02065251 |\n",
      "|    clip_fraction          | 0.208      |\n",
      "|    clip_range             | 0.2        |\n",
      "|    entropy_loss           | -2.57      |\n",
      "|    explained_variance     | -0.1       |\n",
      "|    learning_rate          | 0.0003     |\n",
      "|    loss                   | 0.617      |\n",
      "|    n_updates              | 400        |\n",
      "|    policy_gradient_loss   | -0.05      |\n",
      "|    value_loss             | 4          |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 43136    |\n",
      "|    positive_rewards       | 193      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 40       |\n",
      "|    iterations             | 41       |\n",
      "|    time_elapsed           | 2081     |\n",
      "|    total_timesteps        | 83968    |\n",
      "| time_per_step             | 0        |\n",
      "----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=85000, episode_reward=-9423.80 +/- 4164.59\n",
      "Episode length: 18659.40 +/- 8219.68\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.87e+04    |\n",
      "|    mean_reward            | -9.42e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 43642       |\n",
      "|    positive_rewards       | 194         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [0.]        |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 85000       |\n",
      "| time_per_step             | 0.00101     |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.015022505 |\n",
      "|    clip_fraction          | 0.149       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -2.64       |\n",
      "|    explained_variance     | -0.0259     |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 0.887       |\n",
      "|    n_updates              | 410         |\n",
      "|    policy_gradient_loss   | -0.0403     |\n",
      "|    value_loss             | 8.49        |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 44650    |\n",
      "|    positive_rewards       | 197      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 40       |\n",
      "|    iterations             | 42       |\n",
      "|    time_elapsed           | 2117     |\n",
      "|    total_timesteps        | 86016    |\n",
      "| time_per_step             | 0.000996 |\n",
      "----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=87500, episode_reward=-9565.20 +/- 3486.95\n",
      "Episode length: 18561.20 +/- 6056.29\n",
      "--------------------------------------------\n",
      "| eval/                     |              |\n",
      "|    mean_ep_length         | 1.86e+04     |\n",
      "|    mean_reward            | -9.57e+03    |\n",
      "| exploration_exploitation/ |              |\n",
      "|    negative_rewards       | 45150        |\n",
      "|    positive_rewards       | 199          |\n",
      "| reward/                   |              |\n",
      "|    player_1               | [-10.]       |\n",
      "| time/                     |              |\n",
      "|    total_timesteps        | 87500        |\n",
      "| time_per_step             | 0.000945     |\n",
      "| train/                    |              |\n",
      "|    approx_kl              | 0.0136117935 |\n",
      "|    clip_fraction          | 0.147        |\n",
      "|    clip_range             | 0.2          |\n",
      "|    entropy_loss           | -2.55        |\n",
      "|    explained_variance     | 0.0491       |\n",
      "|    learning_rate          | 0.0003       |\n",
      "|    loss                   | 2.07         |\n",
      "|    n_updates              | 420          |\n",
      "|    policy_gradient_loss   | -0.0391      |\n",
      "|    value_loss             | 7.59         |\n",
      "--------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 45653    |\n",
      "|    positive_rewards       | 201      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 40       |\n",
      "|    iterations             | 43       |\n",
      "|    time_elapsed           | 2155     |\n",
      "|    total_timesteps        | 88064    |\n",
      "| time_per_step             | 0        |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=90000, episode_reward=-6116.20 +/- 1894.66\n",
      "Episode length: 12101.40 +/- 3964.57\n",
      "------------------------------------------\n",
      "| eval/                     |            |\n",
      "|    mean_ep_length         | 1.21e+04   |\n",
      "|    mean_reward            | -6.12e+03  |\n",
      "| exploration_exploitation/ |            |\n",
      "|    negative_rewards       | 46153      |\n",
      "|    positive_rewards       | 203        |\n",
      "| reward/                   |            |\n",
      "|    player_1               | [0.]       |\n",
      "| time/                     |            |\n",
      "|    total_timesteps        | 90000      |\n",
      "| time_per_step             | 0          |\n",
      "| train/                    |            |\n",
      "|    approx_kl              | 0.01808305 |\n",
      "|    clip_fraction          | 0.185      |\n",
      "|    clip_range             | 0.2        |\n",
      "|    entropy_loss           | -2.58      |\n",
      "|    explained_variance     | 0.104      |\n",
      "|    learning_rate          | 0.0003     |\n",
      "|    loss                   | 0.915      |\n",
      "|    n_updates              | 430        |\n",
      "|    policy_gradient_loss   | -0.0442    |\n",
      "|    value_loss             | 4.51       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 46652    |\n",
      "|    positive_rewards       | 205      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 41       |\n",
      "|    iterations             | 44       |\n",
      "|    time_elapsed           | 2179     |\n",
      "|    total_timesteps        | 90112    |\n",
      "| time_per_step             | 0.0011   |\n",
      "----------------------------------------\n",
      "-------------------------------------------\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 47661       |\n",
      "|    positive_rewards       | 208         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [0.]        |\n",
      "| rollout/                  |             |\n",
      "|    ep_len_mean            | 1.84e+03    |\n",
      "|    ep_rew_mean            | -2.2e+03    |\n",
      "| time/                     |             |\n",
      "|    fps                    | 42          |\n",
      "|    iterations             | 45          |\n",
      "|    time_elapsed           | 2182        |\n",
      "|    total_timesteps        | 92160       |\n",
      "| time_per_step             | 0.000618    |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.024961822 |\n",
      "|    clip_fraction          | 0.237       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -2.52       |\n",
      "|    explained_variance     | -0.31       |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 0.815       |\n",
      "|    n_updates              | 440         |\n",
      "|    policy_gradient_loss   | -0.0498     |\n",
      "|    value_loss             | 2.98        |\n",
      "-------------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=92500, episode_reward=-9173.80 +/- 1493.27\n",
      "Episode length: 16411.00 +/- 2083.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.64e+04    |\n",
      "|    mean_reward          | -9.17e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 92500       |\n",
      "| time_per_step           | 0           |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012079429 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.52       |\n",
      "|    explained_variance   | 0.0344      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.61        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0332     |\n",
      "|    value_loss           | 9.97        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 48674    |\n",
      "|    positive_rewards       | 210      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 42       |\n",
      "|    iterations             | 46       |\n",
      "|    time_elapsed           | 2215     |\n",
      "|    total_timesteps        | 94208    |\n",
      "| time_per_step             | 0        |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=95000, episode_reward=-8010.40 +/- 3245.49\n",
      "Episode length: 13264.00 +/- 4368.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.33e+04    |\n",
      "|    mean_reward          | -8.01e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 95000       |\n",
      "| time_per_step           | 0.000998    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012544283 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | -0.0132     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.7         |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    value_loss           | 11.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 49677    |\n",
      "|    positive_rewards       | 212      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [0.]     |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 42       |\n",
      "|    iterations             | 47       |\n",
      "|    time_elapsed           | 2251     |\n",
      "|    total_timesteps        | 96256    |\n",
      "| time_per_step             | 0.00121  |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=97500, episode_reward=-9419.20 +/- 3863.01\n",
      "Episode length: 16520.00 +/- 7376.41\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.65e+04    |\n",
      "|    mean_reward            | -9.42e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 50184       |\n",
      "|    positive_rewards       | 214         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [0.]        |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 97500       |\n",
      "| time_per_step             | 0.000992    |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.016443986 |\n",
      "|    clip_fraction          | 0.171       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -2.55       |\n",
      "|    explained_variance     | -0.183      |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 1.29        |\n",
      "|    n_updates              | 470         |\n",
      "|    policy_gradient_loss   | -0.0438     |\n",
      "|    value_loss             | 8.12        |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 50687    |\n",
      "|    positive_rewards       | 214      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 43       |\n",
      "|    iterations             | 48       |\n",
      "|    time_elapsed           | 2285     |\n",
      "|    total_timesteps        | 98304    |\n",
      "| time_per_step             | 0        |\n",
      "----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=100000, episode_reward=-6112.40 +/- 1171.55\n",
      "Episode length: 11131.20 +/- 3058.40\n",
      "------------------------------------------\n",
      "| eval/                     |            |\n",
      "|    mean_ep_length         | 1.11e+04   |\n",
      "|    mean_reward            | -6.11e+03  |\n",
      "| exploration_exploitation/ |            |\n",
      "|    negative_rewards       | 51188      |\n",
      "|    positive_rewards       | 215        |\n",
      "| reward/                   |            |\n",
      "|    player_1               | [0.]       |\n",
      "| time/                     |            |\n",
      "|    total_timesteps        | 100000     |\n",
      "| time_per_step             | 0.000613   |\n",
      "| train/                    |            |\n",
      "|    approx_kl              | 0.01675105 |\n",
      "|    clip_fraction          | 0.193      |\n",
      "|    clip_range             | 0.2        |\n",
      "|    entropy_loss           | -2.46      |\n",
      "|    explained_variance     | -0.139     |\n",
      "|    learning_rate          | 0.0003     |\n",
      "|    loss                   | 0.612      |\n",
      "|    n_updates              | 480        |\n",
      "|    policy_gradient_loss   | -0.0482    |\n",
      "|    value_loss             | 5.8        |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 51690    |\n",
      "|    positive_rewards       | 215      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 43       |\n",
      "|    iterations             | 49       |\n",
      "|    time_elapsed           | 2308     |\n",
      "|    total_timesteps        | 100352   |\n",
      "| time_per_step             | 0.000998 |\n",
      "----------------------------------------\n",
      "-------------------------------------------\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 52693       |\n",
      "|    positive_rewards       | 218         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [-1.]       |\n",
      "| rollout/                  |             |\n",
      "|    ep_len_mean            | 1.84e+03    |\n",
      "|    ep_rew_mean            | -2.2e+03    |\n",
      "| time/                     |             |\n",
      "|    fps                    | 44          |\n",
      "|    iterations             | 50          |\n",
      "|    time_elapsed           | 2311        |\n",
      "|    total_timesteps        | 102400      |\n",
      "| time_per_step             | 0.001       |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.030684741 |\n",
      "|    clip_fraction          | 0.301       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -2.52       |\n",
      "|    explained_variance     | -0.359      |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 0.254       |\n",
      "|    n_updates              | 490         |\n",
      "|    policy_gradient_loss   | -0.0548     |\n",
      "|    value_loss             | 1.8         |\n",
      "-------------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=102500, episode_reward=-8294.20 +/- 3516.47\n",
      "Episode length: 16232.60 +/- 6737.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.62e+04    |\n",
      "|    mean_reward          | -8.29e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 102500      |\n",
      "| time_per_step           | 0.000998    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014944011 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | -0.0463     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.25        |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0449     |\n",
      "|    value_loss           | 5.64        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 53698    |\n",
      "|    positive_rewards       | 221      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [0.]     |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 44       |\n",
      "|    iterations             | 51       |\n",
      "|    time_elapsed           | 2344     |\n",
      "|    total_timesteps        | 104448   |\n",
      "| time_per_step             | 0.001    |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=105000, episode_reward=-7532.60 +/- 1342.07\n",
      "Episode length: 14490.20 +/- 2839.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.45e+04    |\n",
      "|    mean_reward          | -7.53e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 105000      |\n",
      "| time_per_step           | 0           |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015378131 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.47       |\n",
      "|    explained_variance   | -0.26       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.51        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0449     |\n",
      "|    value_loss           | 5.22        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 54702    |\n",
      "|    positive_rewards       | 225      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 44       |\n",
      "|    iterations             | 52       |\n",
      "|    time_elapsed           | 2375     |\n",
      "|    total_timesteps        | 106496   |\n",
      "| time_per_step             | 0.001    |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=107500, episode_reward=-7187.80 +/- 3371.53\n",
      "Episode length: 13939.40 +/- 6370.23\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.39e+04    |\n",
      "|    mean_reward            | -7.19e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 55200       |\n",
      "|    positive_rewards       | 228         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [-1.]       |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 107500      |\n",
      "| time_per_step             | 0.00101     |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.016625047 |\n",
      "|    clip_fraction          | 0.167       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -2.39       |\n",
      "|    explained_variance     | -0.0734     |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 0.898       |\n",
      "|    n_updates              | 520         |\n",
      "|    policy_gradient_loss   | -0.0453     |\n",
      "|    value_loss             | 5.19        |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 55701    |\n",
      "|    positive_rewards       | 229      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [0.]     |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 45       |\n",
      "|    iterations             | 53       |\n",
      "|    time_elapsed           | 2405     |\n",
      "|    total_timesteps        | 108544   |\n",
      "| time_per_step             | 0.000999 |\n",
      "----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=110000, episode_reward=-8211.80 +/- 2486.67\n",
      "Episode length: 15816.40 +/- 4412.22\n",
      "------------------------------------------\n",
      "| eval/                     |            |\n",
      "|    mean_ep_length         | 1.58e+04   |\n",
      "|    mean_reward            | -8.21e+03  |\n",
      "| exploration_exploitation/ |            |\n",
      "|    negative_rewards       | 56208      |\n",
      "|    positive_rewards       | 230        |\n",
      "| reward/                   |            |\n",
      "|    player_1               | [0.]       |\n",
      "| time/                     |            |\n",
      "|    total_timesteps        | 110000     |\n",
      "| time_per_step             | 0          |\n",
      "| train/                    |            |\n",
      "|    approx_kl              | 0.01473704 |\n",
      "|    clip_fraction          | 0.145      |\n",
      "|    clip_range             | 0.2        |\n",
      "|    entropy_loss           | -2.39      |\n",
      "|    explained_variance     | 0.0246     |\n",
      "|    learning_rate          | 0.0003     |\n",
      "|    loss                   | 2.12       |\n",
      "|    n_updates              | 530        |\n",
      "|    policy_gradient_loss   | -0.039     |\n",
      "|    value_loss             | 7.32       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 56716    |\n",
      "|    positive_rewards       | 231      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [0.]     |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 45       |\n",
      "|    iterations             | 54       |\n",
      "|    time_elapsed           | 2441     |\n",
      "|    total_timesteps        | 110592   |\n",
      "| time_per_step             | 0.00101  |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=112500, episode_reward=-7680.40 +/- 2236.35\n",
      "Episode length: 15102.60 +/- 4267.01\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.51e+04    |\n",
      "|    mean_reward            | -7.68e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 57714       |\n",
      "|    positive_rewards       | 237         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [0.]        |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 112500      |\n",
      "| time_per_step             | 0.000994    |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.010837315 |\n",
      "|    clip_fraction          | 0.0917      |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -2.49       |\n",
      "|    explained_variance     | 0.104       |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 2.8         |\n",
      "|    n_updates              | 540         |\n",
      "|    policy_gradient_loss   | -0.0219     |\n",
      "|    value_loss             | 20.3        |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| reward/            |          |\n",
      "|    player_1        | [0.]     |\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.84e+03 |\n",
      "|    ep_rew_mean     | -2.2e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 2473     |\n",
      "|    total_timesteps | 112640   |\n",
      "| time_per_step      | 0.002    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| exploration_exploitation/ |            |\n",
      "|    negative_rewards       | 58718      |\n",
      "|    positive_rewards       | 239        |\n",
      "| reward/                   |            |\n",
      "|    player_1               | [-1.]      |\n",
      "| rollout/                  |            |\n",
      "|    ep_len_mean            | 1.84e+03   |\n",
      "|    ep_rew_mean            | -2.2e+03   |\n",
      "| time/                     |            |\n",
      "|    fps                    | 46         |\n",
      "|    iterations             | 56         |\n",
      "|    time_elapsed           | 2476       |\n",
      "|    total_timesteps        | 114688     |\n",
      "| time_per_step             | 0          |\n",
      "| train/                    |            |\n",
      "|    approx_kl              | 0.02312677 |\n",
      "|    clip_fraction          | 0.264      |\n",
      "|    clip_range             | 0.2        |\n",
      "|    entropy_loss           | -2.41      |\n",
      "|    explained_variance     | -0.386     |\n",
      "|    learning_rate          | 0.0003     |\n",
      "|    loss                   | 0.823      |\n",
      "|    n_updates              | 550        |\n",
      "|    policy_gradient_loss   | -0.0508    |\n",
      "|    value_loss             | 4.69       |\n",
      "------------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=115000, episode_reward=-6424.00 +/- 1798.28\n",
      "Episode length: 12385.40 +/- 3185.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.24e+04    |\n",
      "|    mean_reward          | -6.42e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-10.]      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 115000      |\n",
      "| time_per_step           | 0.00113     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014322341 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.37       |\n",
      "|    explained_variance   | 0.0806      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.66        |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0346     |\n",
      "|    value_loss           | 6.79        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 59725    |\n",
      "|    positive_rewards       | 243      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 46       |\n",
      "|    iterations             | 57       |\n",
      "|    time_elapsed           | 2504     |\n",
      "|    total_timesteps        | 116736   |\n",
      "| time_per_step             | 0        |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=117500, episode_reward=-7528.00 +/- 2135.12\n",
      "Episode length: 14394.00 +/- 3824.31\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.44e+04    |\n",
      "|    mean_reward            | -7.53e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 60227       |\n",
      "|    positive_rewards       | 244         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [0.]        |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 117500      |\n",
      "| time_per_step             | 0.00101     |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.013628984 |\n",
      "|    clip_fraction          | 0.132       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -2.39       |\n",
      "|    explained_variance     | -0.0956     |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 2.76        |\n",
      "|    n_updates              | 570         |\n",
      "|    policy_gradient_loss   | -0.0369     |\n",
      "|    value_loss             | 8.77        |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 60731    |\n",
      "|    positive_rewards       | 246      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [0.]     |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 46       |\n",
      "|    iterations             | 58       |\n",
      "|    time_elapsed           | 2535     |\n",
      "|    total_timesteps        | 118784   |\n",
      "| time_per_step             | 0.00101  |\n",
      "----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=120000, episode_reward=-5320.20 +/- 2156.67\n",
      "Episode length: 10429.00 +/- 4083.89\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.04e+04    |\n",
      "|    mean_reward            | -5.32e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 61230       |\n",
      "|    positive_rewards       | 248         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [0.]        |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 120000      |\n",
      "| time_per_step             | 0           |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.018358873 |\n",
      "|    clip_fraction          | 0.185       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -2.4        |\n",
      "|    explained_variance     | -0.332      |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 2.69        |\n",
      "|    n_updates              | 580         |\n",
      "|    policy_gradient_loss   | -0.0398     |\n",
      "|    value_loss             | 8.12        |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 61733    |\n",
      "|    positive_rewards       | 248      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [0.]     |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 47       |\n",
      "|    iterations             | 59       |\n",
      "|    time_elapsed           | 2558     |\n",
      "|    total_timesteps        | 120832   |\n",
      "| time_per_step             | 0        |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=122500, episode_reward=-4978.60 +/- 2774.69\n",
      "Episode length: 9462.40 +/- 5221.43\n",
      "------------------------------------------\n",
      "| eval/                     |            |\n",
      "|    mean_ep_length         | 9.46e+03   |\n",
      "|    mean_reward            | -4.98e+03  |\n",
      "| exploration_exploitation/ |            |\n",
      "|    negative_rewards       | 62741      |\n",
      "|    positive_rewards       | 249        |\n",
      "| reward/                   |            |\n",
      "|    player_1               | [-1.]      |\n",
      "| time/                     |            |\n",
      "|    total_timesteps        | 122500     |\n",
      "| time_per_step             | 0.000996   |\n",
      "| train/                    |            |\n",
      "|    approx_kl              | 0.02262618 |\n",
      "|    clip_fraction          | 0.227      |\n",
      "|    clip_range             | 0.2        |\n",
      "|    entropy_loss           | -2.3       |\n",
      "|    explained_variance     | 0.0486     |\n",
      "|    learning_rate          | 0.0003     |\n",
      "|    loss                   | 0.718      |\n",
      "|    n_updates              | 590        |\n",
      "|    policy_gradient_loss   | -0.0432    |\n",
      "|    value_loss             | 3.55       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| reward/            |          |\n",
      "|    player_1        | [0.]     |\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.84e+03 |\n",
      "|    ep_rew_mean     | -2.2e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 47       |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 2579     |\n",
      "|    total_timesteps | 122880   |\n",
      "| time_per_step      | 0.000585 |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| exploration_exploitation/ |            |\n",
      "|    negative_rewards       | 63741      |\n",
      "|    positive_rewards       | 255        |\n",
      "| reward/                   |            |\n",
      "|    player_1               | [-1.]      |\n",
      "| rollout/                  |            |\n",
      "|    ep_len_mean            | 1.84e+03   |\n",
      "|    ep_rew_mean            | -2.2e+03   |\n",
      "| time/                     |            |\n",
      "|    fps                    | 48         |\n",
      "|    iterations             | 61         |\n",
      "|    time_elapsed           | 2581       |\n",
      "|    total_timesteps        | 124928     |\n",
      "| time_per_step             | 0          |\n",
      "| train/                    |            |\n",
      "|    approx_kl              | 0.01624024 |\n",
      "|    clip_fraction          | 0.164      |\n",
      "|    clip_range             | 0.2        |\n",
      "|    entropy_loss           | -2.29      |\n",
      "|    explained_variance     | -0.0375    |\n",
      "|    learning_rate          | 0.0003     |\n",
      "|    loss                   | 1.07       |\n",
      "|    n_updates              | 600        |\n",
      "|    policy_gradient_loss   | -0.0327    |\n",
      "|    value_loss             | 10         |\n",
      "------------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=125000, episode_reward=-8117.80 +/- 3234.53\n",
      "Episode length: 16417.00 +/- 6427.08\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.64e+04   |\n",
      "|    mean_reward          | -8.12e+03  |\n",
      "| reward/                 |            |\n",
      "|    player_1             | [0.]       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 125000     |\n",
      "| time_per_step           | 0          |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01572401 |\n",
      "|    clip_fraction        | 0.181      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.34      |\n",
      "|    explained_variance   | 0.0199     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.899      |\n",
      "|    n_updates            | 610        |\n",
      "|    policy_gradient_loss | -0.0409    |\n",
      "|    value_loss           | 6.77       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 64742    |\n",
      "|    positive_rewards       | 261      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [0.]     |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 48       |\n",
      "|    iterations             | 62       |\n",
      "|    time_elapsed           | 2616     |\n",
      "|    total_timesteps        | 126976   |\n",
      "| time_per_step             | 0        |\n",
      "----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=127500, episode_reward=-8648.20 +/- 3706.77\n",
      "Episode length: 17252.80 +/- 7222.73\n",
      "------------------------------------------\n",
      "| eval/                     |            |\n",
      "|    mean_ep_length         | 1.73e+04   |\n",
      "|    mean_reward            | -8.65e+03  |\n",
      "| exploration_exploitation/ |            |\n",
      "|    negative_rewards       | 65240      |\n",
      "|    positive_rewards       | 264        |\n",
      "| reward/                   |            |\n",
      "|    player_1               | [-1.]      |\n",
      "| time/                     |            |\n",
      "|    total_timesteps        | 127500     |\n",
      "| time_per_step             | 0.001      |\n",
      "| train/                    |            |\n",
      "|    approx_kl              | 0.02189684 |\n",
      "|    clip_fraction          | 0.27       |\n",
      "|    clip_range             | 0.2        |\n",
      "|    entropy_loss           | -2.21      |\n",
      "|    explained_variance     | -0.21      |\n",
      "|    learning_rate          | 0.0003     |\n",
      "|    loss                   | 0.331      |\n",
      "|    n_updates              | 620        |\n",
      "|    policy_gradient_loss   | -0.05      |\n",
      "|    value_loss             | 3.23       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 66241    |\n",
      "|    positive_rewards       | 268      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 48       |\n",
      "|    iterations             | 63       |\n",
      "|    time_elapsed           | 2650     |\n",
      "|    total_timesteps        | 129024   |\n",
      "| time_per_step             | 0.00113  |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=130000, episode_reward=-8650.40 +/- 1657.04\n",
      "Episode length: 16893.00 +/- 2982.78\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.69e+04    |\n",
      "|    mean_reward          | -8.65e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 130000      |\n",
      "| time_per_step           | 0.001       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021111945 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.29       |\n",
      "|    explained_variance   | -0.177      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.54        |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0509     |\n",
      "|    value_loss           | 4.28        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 67244    |\n",
      "|    positive_rewards       | 271      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 48       |\n",
      "|    iterations             | 64       |\n",
      "|    time_elapsed           | 2688     |\n",
      "|    total_timesteps        | 131072   |\n",
      "| time_per_step             | 0        |\n",
      "----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=132500, episode_reward=-6736.40 +/- 1661.96\n",
      "Episode length: 12789.60 +/- 3070.59\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.28e+04    |\n",
      "|    mean_reward            | -6.74e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 67745       |\n",
      "|    positive_rewards       | 272         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [-1.]       |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 132500      |\n",
      "| time_per_step             | 0           |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.017457021 |\n",
      "|    clip_fraction          | 0.189       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -2.23       |\n",
      "|    explained_variance     | 0.0388      |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 0.347       |\n",
      "|    n_updates              | 640         |\n",
      "|    policy_gradient_loss   | -0.0415     |\n",
      "|    value_loss             | 5.33        |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 68245    |\n",
      "|    positive_rewards       | 272      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [0.]     |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 49       |\n",
      "|    iterations             | 65       |\n",
      "|    time_elapsed           | 2714     |\n",
      "|    total_timesteps        | 133120   |\n",
      "| time_per_step             | 0.001    |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=135000, episode_reward=-8516.40 +/- 4032.76\n",
      "Episode length: 16232.40 +/- 7256.01\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.62e+04    |\n",
      "|    mean_reward            | -8.52e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 68749       |\n",
      "|    positive_rewards       | 272         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [0.]        |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 135000      |\n",
      "| time_per_step             | 0           |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.025258422 |\n",
      "|    clip_fraction          | 0.233       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -2.27       |\n",
      "|    explained_variance     | -0.0486     |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 0.461       |\n",
      "|    n_updates              | 650         |\n",
      "|    policy_gradient_loss   | -0.0435     |\n",
      "|    value_loss             | 5.31        |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 69252    |\n",
      "|    positive_rewards       | 272      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 49       |\n",
      "|    iterations             | 66       |\n",
      "|    time_elapsed           | 2752     |\n",
      "|    total_timesteps        | 135168   |\n",
      "| time_per_step             | 0.000993 |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| exploration_exploitation/ |            |\n",
      "|    negative_rewards       | 70259      |\n",
      "|    positive_rewards       | 275        |\n",
      "| reward/                   |            |\n",
      "|    player_1               | [0.]       |\n",
      "| rollout/                  |            |\n",
      "|    ep_len_mean            | 1.84e+03   |\n",
      "|    ep_rew_mean            | -2.2e+03   |\n",
      "| time/                     |            |\n",
      "|    fps                    | 49         |\n",
      "|    iterations             | 67         |\n",
      "|    time_elapsed           | 2755       |\n",
      "|    total_timesteps        | 137216     |\n",
      "| time_per_step             | 0.000987   |\n",
      "| train/                    |            |\n",
      "|    approx_kl              | 0.02281452 |\n",
      "|    clip_fraction          | 0.246      |\n",
      "|    clip_range             | 0.2        |\n",
      "|    entropy_loss           | -2.22      |\n",
      "|    explained_variance     | 0.0312     |\n",
      "|    learning_rate          | 0.0003     |\n",
      "|    loss                   | 0.597      |\n",
      "|    n_updates              | 660        |\n",
      "|    policy_gradient_loss   | -0.0474    |\n",
      "|    value_loss             | 3.04       |\n",
      "------------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=137500, episode_reward=-7090.80 +/- 840.30\n",
      "Episode length: 14068.20 +/- 1671.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.41e+04    |\n",
      "|    mean_reward          | -7.09e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [0.]        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 137500      |\n",
      "| time_per_step           | 0.001       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011465969 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.27       |\n",
      "|    explained_variance   | 0.0712      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.82        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    value_loss           | 14          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 71260    |\n",
      "|    positive_rewards       | 277      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [0.]     |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 50       |\n",
      "|    iterations             | 68       |\n",
      "|    time_elapsed           | 2785     |\n",
      "|    total_timesteps        | 139264   |\n",
      "| time_per_step             | 0.00101  |\n",
      "----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=140000, episode_reward=-7041.40 +/- 1646.33\n",
      "Episode length: 14199.40 +/- 3251.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.42e+04    |\n",
      "|    mean_reward          | -7.04e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 140000      |\n",
      "| time_per_step           | 0.000999    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032518357 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.18       |\n",
      "|    explained_variance   | -0.264      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.119       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.055      |\n",
      "|    value_loss           | 1.61        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 72261    |\n",
      "|    positive_rewards       | 279      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [0.]     |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 50       |\n",
      "|    iterations             | 69       |\n",
      "|    time_elapsed           | 2816     |\n",
      "|    total_timesteps        | 141312   |\n",
      "| time_per_step             | 0.001    |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=142500, episode_reward=-6324.60 +/- 1557.24\n",
      "Episode length: 12393.00 +/- 3083.17\n",
      "------------------------------------------\n",
      "| eval/                     |            |\n",
      "|    mean_ep_length         | 1.24e+04   |\n",
      "|    mean_reward            | -6.32e+03  |\n",
      "| exploration_exploitation/ |            |\n",
      "|    negative_rewards       | 72763      |\n",
      "|    positive_rewards       | 279        |\n",
      "| reward/                   |            |\n",
      "|    player_1               | [-1.]      |\n",
      "| time/                     |            |\n",
      "|    total_timesteps        | 142500     |\n",
      "| time_per_step             | 0.000998   |\n",
      "| train/                    |            |\n",
      "|    approx_kl              | 0.02110983 |\n",
      "|    clip_fraction          | 0.243      |\n",
      "|    clip_range             | 0.2        |\n",
      "|    entropy_loss           | -2.15      |\n",
      "|    explained_variance     | -0.0624    |\n",
      "|    learning_rate          | 0.0003     |\n",
      "|    loss                   | 0.476      |\n",
      "|    n_updates              | 690        |\n",
      "|    policy_gradient_loss   | -0.0468    |\n",
      "|    value_loss             | 2.3        |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 73264    |\n",
      "|    positive_rewards       | 280      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 50       |\n",
      "|    iterations             | 70       |\n",
      "|    time_elapsed           | 2842     |\n",
      "|    total_timesteps        | 143360   |\n",
      "| time_per_step             | 0.001    |\n",
      "----------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Eval num_timesteps=145000, episode_reward=-6935.20 +/- 625.83\n",
      "Episode length: 13965.60 +/- 1316.84\n",
      "-------------------------------------------\n",
      "| eval/                     |             |\n",
      "|    mean_ep_length         | 1.4e+04     |\n",
      "|    mean_reward            | -6.94e+03   |\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 73765       |\n",
      "|    positive_rewards       | 281         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [0.]        |\n",
      "| time/                     |             |\n",
      "|    total_timesteps        | 145000      |\n",
      "| time_per_step             | 0           |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.018658731 |\n",
      "|    clip_fraction          | 0.193       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -2.1        |\n",
      "|    explained_variance     | -0.0448     |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 0.986       |\n",
      "|    n_updates              | 700         |\n",
      "|    policy_gradient_loss   | -0.039      |\n",
      "|    value_loss             | 3.34        |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 74265    |\n",
      "|    positive_rewards       | 282      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 50       |\n",
      "|    iterations             | 71       |\n",
      "|    time_elapsed           | 2872     |\n",
      "|    total_timesteps        | 145408   |\n",
      "| time_per_step             | 0        |\n",
      "----------------------------------------\n",
      "-------------------------------------------\n",
      "| exploration_exploitation/ |             |\n",
      "|    negative_rewards       | 75265       |\n",
      "|    positive_rewards       | 284         |\n",
      "| reward/                   |             |\n",
      "|    player_1               | [0.]        |\n",
      "| rollout/                  |             |\n",
      "|    ep_len_mean            | 1.84e+03    |\n",
      "|    ep_rew_mean            | -2.2e+03    |\n",
      "| time/                     |             |\n",
      "|    fps                    | 51          |\n",
      "|    iterations             | 72          |\n",
      "|    time_elapsed           | 2875        |\n",
      "|    total_timesteps        | 147456      |\n",
      "| time_per_step             | 0.000997    |\n",
      "| train/                    |             |\n",
      "|    approx_kl              | 0.024861982 |\n",
      "|    clip_fraction          | 0.226       |\n",
      "|    clip_range             | 0.2         |\n",
      "|    entropy_loss           | -2.04       |\n",
      "|    explained_variance     | -0.0862     |\n",
      "|    learning_rate          | 0.0003      |\n",
      "|    loss                   | 0.197       |\n",
      "|    n_updates              | 710         |\n",
      "|    policy_gradient_loss   | -0.044      |\n",
      "|    value_loss             | 1.77        |\n",
      "-------------------------------------------\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=147500, episode_reward=-9986.00 +/- 5248.76\n",
      "Episode length: 20194.40 +/- 10515.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.02e+04    |\n",
      "|    mean_reward          | -9.99e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 147500      |\n",
      "| time_per_step           | 0.00104     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023770254 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.05       |\n",
      "|    explained_variance   | -0.0152     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.124       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0421     |\n",
      "|    value_loss           | 1.37        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 76265    |\n",
      "|    positive_rewards       | 286      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 51       |\n",
      "|    iterations             | 73       |\n",
      "|    time_elapsed           | 2916     |\n",
      "|    total_timesteps        | 149504   |\n",
      "| time_per_step             | 0.00161  |\n",
      "----------------------------------------\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 1 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Le joueur 0 a gagné la partie !\n",
      "Eval num_timesteps=150000, episode_reward=-8033.20 +/- 2849.75\n",
      "Episode length: 16108.20 +/- 5708.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.61e+04    |\n",
      "|    mean_reward          | -8.03e+03   |\n",
      "| reward/                 |             |\n",
      "|    player_1             | [-1.]       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 150000      |\n",
      "| time_per_step           | 0.00101     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015872931 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.06       |\n",
      "|    explained_variance   | 0.006       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.557       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    value_loss           | 2.39        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| exploration_exploitation/ |          |\n",
      "|    negative_rewards       | 77268    |\n",
      "|    positive_rewards       | 287      |\n",
      "| reward/                   |          |\n",
      "|    player_1               | [-1.]    |\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 1.84e+03 |\n",
      "|    ep_rew_mean            | -2.2e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 51       |\n",
      "|    iterations             | 74       |\n",
      "|    time_elapsed           | 2948     |\n",
      "|    total_timesteps        | 151552   |\n",
      "| time_per_step             | 0.00153  |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x23eb0e81610>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_logging_callback = RewardLoggingCallback()\n",
    "exploration_exploitation_callback = ExplorationExploitationCallback()\n",
    "save_model_callback = SaveModelCallback(save_freq=2500, save_path=\"./modeles/agent_3\")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_dir)\n",
    "model.learn(\n",
    "    total_timesteps=150000,\n",
    "    callback=[\n",
    "        eval_callback,\n",
    "        reward_logging_callback,\n",
    "        save_model_callback,\n",
    "        exploration_exploitation_callback,\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info, _ = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"labyrinth_agent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
